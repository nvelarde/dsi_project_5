{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.project_5 import general_process, load_data_from_database, make_data_dict, general_model, general_transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**NOTE: EACH OF THESE SHOULD BE WRITTEN SOLELY WITH REGARD TO STEP 1 - BENCHMARKING**\n",
    "\n",
    "### Domain and Data\n",
    "\n",
    "**TODO:** Write a simple statement about the domain of your problem and the dataset upon which you will be working. \n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "**TODO:** Write a simple problem statement with regard to benchmarking your work only.\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "**TODO:** Write a simple solution statement with regard to benchmarking your work only.\n",
    "\n",
    "### Metric\n",
    "\n",
    "**TODO**: Write a statement about the metric you will be using. This section is global as it will be the metric you will use throughout this project. Provide a brief justification for choosing this metric.\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "**TODO**: Write a statement explaining that this is the process by which you identify a benchmark for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Implement the following code pipeline using the functions you write in `lib/project_5.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/benchmarking.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "madelon_df = load_data_from_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>feat_000</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>537</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>471</td>\n",
       "      <td>491</td>\n",
       "      <td>476</td>\n",
       "      <td>475</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>477</td>\n",
       "      <td>485</td>\n",
       "      <td>511</td>\n",
       "      <td>485</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>475</td>\n",
       "      <td>496</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>483</td>\n",
       "      <td>458</td>\n",
       "      <td>460</td>\n",
       "      <td>487</td>\n",
       "      <td>587</td>\n",
       "      <td>475</td>\n",
       "      <td>526</td>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>...</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>338</td>\n",
       "      <td>513</td>\n",
       "      <td>486</td>\n",
       "      <td>483</td>\n",
       "      <td>492</td>\n",
       "      <td>510</td>\n",
       "      <td>517</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>487</td>\n",
       "      <td>542</td>\n",
       "      <td>499</td>\n",
       "      <td>468</td>\n",
       "      <td>448</td>\n",
       "      <td>471</td>\n",
       "      <td>442</td>\n",
       "      <td>478</td>\n",
       "      <td>480</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>492</td>\n",
       "      <td>650</td>\n",
       "      <td>506</td>\n",
       "      <td>501</td>\n",
       "      <td>480</td>\n",
       "      <td>489</td>\n",
       "      <td>499</td>\n",
       "      <td>498</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>480</td>\n",
       "      <td>491</td>\n",
       "      <td>510</td>\n",
       "      <td>485</td>\n",
       "      <td>495</td>\n",
       "      <td>472</td>\n",
       "      <td>417</td>\n",
       "      <td>474</td>\n",
       "      <td>502</td>\n",
       "      <td>...</td>\n",
       "      <td>480</td>\n",
       "      <td>474</td>\n",
       "      <td>572</td>\n",
       "      <td>454</td>\n",
       "      <td>469</td>\n",
       "      <td>475</td>\n",
       "      <td>482</td>\n",
       "      <td>494</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>484</td>\n",
       "      <td>502</td>\n",
       "      <td>528</td>\n",
       "      <td>489</td>\n",
       "      <td>466</td>\n",
       "      <td>481</td>\n",
       "      <td>402</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>435</td>\n",
       "      <td>486</td>\n",
       "      <td>508</td>\n",
       "      <td>481</td>\n",
       "      <td>504</td>\n",
       "      <td>495</td>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  feat_000  feat_001  feat_002  feat_003  feat_004  feat_005  \\\n",
       "0      0       485       477       537       479       452       471   \n",
       "1      1       483       458       460       487       587       475   \n",
       "2      2       487       542       499       468       448       471   \n",
       "3      3       480       491       510       485       495       472   \n",
       "4      4       484       502       528       489       466       481   \n",
       "\n",
       "   feat_006  feat_007  feat_008  ...    feat_491  feat_492  feat_493  \\\n",
       "0       491       476       475  ...         481       477       485   \n",
       "1       526       479       485  ...         478       487       338   \n",
       "2       442       478       480  ...         481       492       650   \n",
       "3       417       474       502  ...         480       474       572   \n",
       "4       402       478       487  ...         479       452       435   \n",
       "\n",
       "   feat_494  feat_495  feat_496  feat_497  feat_498  feat_499  label  \n",
       "0       511       485       481       479       475       496     -1  \n",
       "1       513       486       483       492       510       517     -1  \n",
       "2       506       501       480       489       499       498     -1  \n",
       "3       454       469       475       482       494       461      1  \n",
       "4       486       508       481       504       495       511      1  \n",
       "\n",
       "[5 rows x 502 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madelon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 502 entries, index to label\n",
      "dtypes: int64(502)\n",
      "memory usage: 7.7 MB\n"
     ]
    }
   ],
   "source": [
    "madelon_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 502)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madelon_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating our target vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = madelon_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target vector is comprised of '1's and '-1's\n",
    "\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    1000\n",
       "-1    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target vector is evenly distributed\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the feature matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_000</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_490</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>537</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>471</td>\n",
       "      <td>491</td>\n",
       "      <td>476</td>\n",
       "      <td>475</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>477</td>\n",
       "      <td>481</td>\n",
       "      <td>477</td>\n",
       "      <td>485</td>\n",
       "      <td>511</td>\n",
       "      <td>485</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>475</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_000  feat_001  feat_002  feat_003  feat_004  feat_005  feat_006  \\\n",
       "0       485       477       537       479       452       471       491   \n",
       "\n",
       "   feat_007  feat_008  feat_009    ...     feat_490  feat_491  feat_492  \\\n",
       "0       476       475       473    ...          477       481       477   \n",
       "\n",
       "   feat_493  feat_494  feat_495  feat_496  feat_497  feat_498  feat_499  \n",
       "0       485       511       485       481       479       475       496  \n",
       "\n",
       "[1 rows x 500 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = madelon_df.drop(['label', 'index'], axis=1)\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirming that our target vector and feature matrix have the appropriate shapes for sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 500), (2000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making data dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "madelon_data_dict = make_data_dict(X, y, 0.25, 82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirming the Train-Test-Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(madelon_data_dict['X_train']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(madelon_data_dict['X_test']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Standard Scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "madelon_scaled_dict = general_transformer(StandardScaler(), madelon_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StandardScaler(copy=True, with_mean=True, with_std=True)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madelon_data_dict['processes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = madelon_scaled_dict['transformer']\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 481.688     ,  483.64066667,  511.44533333,  483.40933333,\n",
       "        501.95666667,  479.336     ,  479.81266667,  476.52933333,\n",
       "        486.79133333,  478.574     ,  486.248     ,  490.96533333,\n",
       "        478.74133333,  482.70666667,  485.17133333,  479.906     ,\n",
       "        479.16266667,  494.94533333,  483.51066667,  477.47333333,\n",
       "        484.47666667,  494.24733333,  476.426     ,  479.38733333,\n",
       "        500.00666667,  504.08333333,  484.518     ,  482.242     ,\n",
       "        480.548     ,  491.65733333,  481.92666667,  499.77333333,\n",
       "        500.84733333,  490.66333333,  486.40866667,  489.25      ,\n",
       "        479.28866667,  480.64666667,  479.73333333,  476.12266667,\n",
       "        476.49666667,  489.59666667,  476.30466667,  487.72933333,\n",
       "        500.57466667,  480.58      ,  496.20933333,  494.418     ,\n",
       "        485.254     ,  479.01733333,  517.442     ,  504.33866667,\n",
       "        484.84933333,  479.952     ,  496.92733333,  494.98133333,\n",
       "        480.36933333,  485.288     ,  509.16133333,  477.61933333,\n",
       "        488.198     ,  481.54866667,  493.63533333,  478.7       ,\n",
       "        499.35533333,  511.04133333,  503.72133333,  490.49533333,\n",
       "        480.94933333,  476.18666667,  514.46666667,  488.558     ,\n",
       "        481.28266667,  484.40066667,  480.59266667,  501.868     ,\n",
       "        478.98      ,  483.65133333,  512.93666667,  482.49      ,\n",
       "        490.49533333,  512.02066667,  483.506     ,  482.27066667,\n",
       "        504.04733333,  488.56533333,  477.06      ,  482.908     ,\n",
       "        479.25933333,  512.758     ,  476.04066667,  507.60733333,\n",
       "        479.43333333,  492.458     ,  480.40133333,  497.918     ,\n",
       "        479.27666667,  479.462     ,  477.08333333,  480.83066667,\n",
       "        479.94266667,  480.69933333,  480.99733333,  477.88333333,\n",
       "        483.84333333,  500.27533333,  484.49933333,  490.61533333,\n",
       "        500.28      ,  480.98333333,  487.78466667,  478.106     ,\n",
       "        477.19733333,  492.512     ,  479.22866667,  478.78333333,\n",
       "        489.04066667,  482.75133333,  496.37733333,  488.598     ,\n",
       "        476.13133333,  478.90666667,  480.58066667,  482.046     ,\n",
       "        478.75266667,  504.81133333,  483.35733333,  499.204     ,\n",
       "        478.96133333,  483.46      ,  501.23466667,  486.122     ,\n",
       "        487.64      ,  477.59666667,  484.668     ,  503.26733333,\n",
       "        498.53533333,  491.36533333,  479.63466667,  483.136     ,\n",
       "        493.02266667,  508.22533333,  482.10533333,  483.38266667,\n",
       "        483.026     ,  487.06933333,  479.84266667,  497.29666667,\n",
       "        480.02733333,  497.138     ,  487.98133333,  483.86066667,\n",
       "        482.02533333,  501.61266667,  476.94      ,  486.89066667,\n",
       "        483.62333333,  477.88333333,  499.276     ,  499.76933333,\n",
       "        478.19933333,  478.56466667,  481.22266667,  479.982     ,\n",
       "        489.06266667,  487.27466667,  476.03666667,  479.13666667,\n",
       "        476.866     ,  511.44      ,  494.60333333,  510.146     ,\n",
       "        507.03266667,  477.09866667,  476.496     ,  500.18066667,\n",
       "        482.878     ,  480.432     ,  483.86666667,  505.904     ,\n",
       "        503.46866667,  514.38533333,  476.67066667,  478.524     ,\n",
       "        488.24466667,  487.26533333,  497.33866667,  492.54666667,\n",
       "        517.774     ,  478.45333333,  481.888     ,  490.59466667,\n",
       "        495.784     ,  506.382     ,  513.25933333,  484.09733333,\n",
       "        479.21733333,  483.20666667,  476.48866667,  486.16866667,\n",
       "        490.86666667,  488.426     ,  489.66266667,  507.99933333,\n",
       "        488.086     ,  476.04933333,  506.38733333,  479.11333333,\n",
       "        481.26266667,  490.58533333,  487.598     ,  492.34733333,\n",
       "        480.51533333,  488.424     ,  480.31666667,  503.218     ,\n",
       "        504.15533333,  480.63266667,  487.31933333,  485.052     ,\n",
       "        478.81666667,  498.9       ,  493.416     ,  487.928     ,\n",
       "        493.38466667,  500.48      ,  485.02933333,  477.52866667,\n",
       "        476.36333333,  493.88066667,  479.85266667,  482.27266667,\n",
       "        478.52533333,  484.126     ,  491.32266667,  496.31266667,\n",
       "        483.10666667,  478.05      ,  475.66266667,  478.06733333,\n",
       "        497.254     ,  498.396     ,  490.16      ,  479.304     ,\n",
       "        507.95066667,  510.78133333,  477.84466667,  479.138     ,\n",
       "        481.80133333,  499.14066667,  489.924     ,  494.246     ,\n",
       "        476.88733333,  508.28      ,  478.32066667,  492.21266667,\n",
       "        499.58466667,  490.97      ,  485.49866667,  485.68266667,\n",
       "        490.838     ,  478.55      ,  494.16133333,  484.146     ,\n",
       "        479.03466667,  494.184     ,  501.076     ,  489.34066667,\n",
       "        484.73266667,  492.50266667,  483.98733333,  499.51666667,\n",
       "        484.35466667,  487.02666667,  476.73866667,  484.42533333,\n",
       "        476.16933333,  495.58066667,  495.92533333,  493.09733333,\n",
       "        476.884     ,  479.466     ,  481.94666667,  476.03533333,\n",
       "        504.17733333,  491.186     ,  478.39066667,  483.91666667,\n",
       "        479.72733333,  493.268     ,  491.45133333,  503.81066667,\n",
       "        496.40866667,  476.29266667,  485.71466667,  497.82666667,\n",
       "        475.20333333,  501.00733333,  497.27533333,  496.22533333,\n",
       "        481.562     ,  489.69133333,  477.96066667,  491.558     ,\n",
       "        494.34466667,  485.75333333,  480.448     ,  480.82866667,\n",
       "        499.16333333,  476.32733333,  486.31466667,  480.12066667,\n",
       "        500.76133333,  499.51      ,  480.98      ,  487.61133333,\n",
       "        477.474     ,  481.19466667,  486.20866667,  511.552     ,\n",
       "        483.176     ,  483.36466667,  490.53866667,  480.35266667,\n",
       "        489.854     ,  480.05066667,  482.48      ,  482.35266667,\n",
       "        486.362     ,  515.292     ,  487.28333333,  479.21066667,\n",
       "        476.15466667,  488.05733333,  495.828     ,  487.788     ,\n",
       "        482.06533333,  487.51133333,  501.602     ,  501.00866667,\n",
       "        506.15466667,  493.742     ,  496.19533333,  502.842     ,\n",
       "        499.69      ,  482.78133333,  481.38666667,  490.21666667,\n",
       "        481.14333333,  503.85933333,  481.358     ,  478.43      ,\n",
       "        484.42333333,  482.27333333,  493.432     ,  484.59933333,\n",
       "        502.87666667,  476.534     ,  506.578     ,  498.5       ,\n",
       "        482.98333333,  491.082     ,  478.186     ,  486.222     ,\n",
       "        480.89266667,  494.91      ,  495.53266667,  483.72066667,\n",
       "        484.468     ,  499.636     ,  487.61266667,  490.04866667,\n",
       "        497.30266667,  487.60466667,  501.018     ,  491.53733333,\n",
       "        480.41866667,  483.44266667,  496.462     ,  481.53133333,\n",
       "        480.884     ,  479.252     ,  496.966     ,  483.96666667,\n",
       "        479.864     ,  478.14733333,  492.27666667,  477.20133333,\n",
       "        494.9       ,  484.04      ,  476.826     ,  493.16533333,\n",
       "        476.39733333,  486.92066667,  478.098     ,  484.28266667,\n",
       "        484.19466667,  482.50266667,  510.644     ,  481.24733333,\n",
       "        476.99333333,  488.90333333,  476.05466667,  487.47466667,\n",
       "        476.64266667,  477.19      ,  481.77866667,  477.90333333,\n",
       "        483.20266667,  478.12866667,  489.77666667,  490.5       ,\n",
       "        488.19866667,  484.06733333,  492.348     ,  517.50933333,\n",
       "        484.17533333,  506.98133333,  510.858     ,  496.908     ,\n",
       "        480.47266667,  478.93066667,  477.76533333,  476.05866667,\n",
       "        485.03733333,  489.122     ,  490.35866667,  484.69933333,\n",
       "        483.966     ,  476.91533333,  493.514     ,  505.48533333,\n",
       "        479.41066667,  498.504     ,  489.03266667,  499.82666667,\n",
       "        477.46733333,  516.87533333,  484.752     ,  485.00733333,\n",
       "        490.53733333,  479.76733333,  495.82533333,  502.00533333,\n",
       "        492.81333333,  477.232     ,  479.628     ,  486.18066667,\n",
       "        484.188     ,  476.402     ,  487.82333333,  477.54266667,\n",
       "        489.414     ,  481.006     ,  479.84      ,  502.994     ,\n",
       "        479.71      ,  481.85      ,  491.18933333,  485.418     ,\n",
       "        503.222     ,  483.192     ,  493.56533333,  480.532     ,\n",
       "        485.39533333,  490.75      ,  480.70266667,  511.02066667,\n",
       "        481.54      ,  481.63      ,  515.904     ,  478.73666667,\n",
       "        480.49466667,  477.628     ,  479.37733333,  485.80133333,\n",
       "        481.46933333,  487.944     ,  483.22466667,  486.79533333,\n",
       "        485.45333333,  478.378     ,  477.046     ,  497.43866667,\n",
       "        502.51533333,  482.84866667,  478.63533333,  483.432     ,\n",
       "        489.878     ,  479.63866667,  482.54266667,  478.75      ,\n",
       "        486.15133333,  495.59      ,  494.02266667,  510.23666667,\n",
       "        478.2       ,  482.94      ,  507.65666667,  490.04866667])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test of General Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_reg_dict = general_model(LogisticRegression(C=1E10, penalty='l1', n_jobs=-1), madelon_scaled_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " LogisticRegression(C=10000000000.0, class_weight=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_dict['processes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000000000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_dict['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a Benchmark Score\n",
    "\n",
    "Train and Test Score from \"out-of-the-box\" Logistic Regression\n",
    "- no Regularlization (C=1E10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.7953\n",
      "Test Score:  0.5260\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Score: {:.4f}\".format(log_reg_dict['train_score']))\n",
    "print(\"Test Score:  {:.4f}\".format(log_reg_dict['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examining the coefficients from Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abscoef</th>\n",
       "      <th>coef</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1.959267</td>\n",
       "      <td>1.959267</td>\n",
       "      <td>feat_475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.765176</td>\n",
       "      <td>1.765176</td>\n",
       "      <td>feat_048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>1.667132</td>\n",
       "      <td>-1.667132</td>\n",
       "      <td>feat_442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>1.495201</td>\n",
       "      <td>-1.495201</td>\n",
       "      <td>feat_318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>1.430702</td>\n",
       "      <td>-1.430702</td>\n",
       "      <td>feat_281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      abscoef      coef  variable\n",
       "475  1.959267  1.959267  feat_475\n",
       "48   1.765176  1.765176  feat_048\n",
       "442  1.667132 -1.667132  feat_442\n",
       "318  1.495201 -1.495201  feat_318\n",
       "281  1.430702 -1.430702  feat_281"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_coef_df = pd.DataFrame({'coef': log_reg_dict['model'].coef_[0],\n",
    "                                'variable': X.columns,\n",
    "                                'abscoef': np.abs(log_reg_dict['model'].coef_[0])\n",
    "                               })\n",
    "\n",
    "log_reg_coef_df.sort_values('abscoef', ascending=False, inplace=True)\n",
    "log_reg_coef_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_coef_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm all features retained -- no regularization given C=1E10\n",
    "\n",
    "log_reg_coef_df[log_reg_coef_df['coef'] != 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pipeline 2 - Identifying Features via Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "madelon_scaled_dict_copy = madelon_scaled_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_list = [0.0001, 0.055, 0.01, 0.1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_score': 0.52800000000000002, 'C': 1, 'train_score': 0.77733333333333332}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for C in C_list:\n",
    "    \n",
    "    general_model(LogisticRegression(C=C, penalty='l1', n_jobs=-1), madelon_scaled_dict_copy)\n",
    "    results['C'] = C\n",
    "    results['train_score'] = madelon_scaled_dict_copy['train_score']\n",
    "    results['test_score'] = madelon_scaled_dict_copy['test_score']\n",
    "\n",
    "    print (results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_reg_dict1 = general_model(LogisticRegression(C=0.0001, penalty='l1', n_jobs=-1), madelon_scaled_dict_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " LogisticRegression(C=10000000000.0, class_weight=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_dict1['processes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_dict1['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.5047\n",
      "Test Score:  0.4860\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Score: {:.4f}\".format(log_reg_dict1['train_score']))\n",
    "print(\"Test Score:  {:.4f}\".format(log_reg_dict1['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examining the Impact of Increasing Regularization on Coefficients -- Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abscoef</th>\n",
       "      <th>coef</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>feat_000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>feat_329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>feat_342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>feat_341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>feat_340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     abscoef  coef  variable\n",
       "0        0.0   0.0  feat_000\n",
       "329      0.0   0.0  feat_329\n",
       "342      0.0   0.0  feat_342\n",
       "341      0.0   0.0  feat_341\n",
       "340      0.0   0.0  feat_340"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_coef1_df = pd.DataFrame({'coef': log_reg_dict1['model'].coef_[0],\n",
    "                                 'variable': X.columns,\n",
    "                                 'abscoef': np.abs(log_reg_dict1['model'].coef_[0])\n",
    "                                })\n",
    "\n",
    "log_reg_coef1_df.sort_values('abscoef', ascending=False, inplace=True)\n",
    "log_reg_coef1_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abscoef     500\n",
       "coef        500\n",
       "variable    500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_features_removed = log_reg_coef1_df[log_reg_coef1_df['coef'] == 0]\n",
    "lasso_features_removed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Baseline Values: Train: 0.7953, Test: 0.5280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C=1: Train: 0.7767, Test: 0.5280 -- 31 features removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C=0.1: Train: 0.7487, Test: 0.5860 -- 235 features removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C=0.01: Train: 0.6180, Test: 0.6060 -- 499 features removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C=0.055: Train: 0.7013, Test: 0.5780 -- 349 features removed -- split the difference between the last case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C=0.0001: Train: 0.5047, Test: 0.4860 -- effectively all of the features removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = [general_model(LogisticRegression(C=C, penalty='l1', n_jobs=-1), madelon_scaled_dict_copy) \\\n",
    "           for C in C_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'X_test': array([[-1.35028888, -0.45361972, -0.83425415, ..., -0.66200238,\n",
       "          -1.09247646, -0.65678969],\n",
       "         [-0.10692895,  0.27798923, -0.93710458, ...,  0.52278935,\n",
       "          -1.09247646,  1.61615002],\n",
       "         [ 0.51475101, -0.38710982,  0.63136453, ...,  1.26328418,\n",
       "           0.57351208,  1.88582084],\n",
       "         ..., \n",
       "         [-0.88402891,  1.54167742,  1.71129407, ..., -0.143656  ,\n",
       "           1.70208496,  0.11369835],\n",
       "         [ 0.35933102, -1.5842881 ,  1.4798806 , ..., -0.81010134,\n",
       "           0.41228738, -0.23302127],\n",
       "         [ 0.35933102,  1.17587295,  0.65707713, ...,  0.22659142,\n",
       "          -0.34009454,  1.34647921]]),\n",
       "  'X_train': array([[ 1.13643097,  0.61053876, -0.21715155, ..., -0.36580445,\n",
       "          -1.76424604,  0.57599117],\n",
       "         [ 0.20391103, -0.58663953,  1.96842015, ...,  0.59683883,\n",
       "           1.64834339, -1.46580213],\n",
       "         [ 0.04849104,  0.31124418,  1.09419147, ..., -0.88415083,\n",
       "           1.27215243,  0.57599117],\n",
       "         ..., \n",
       "         [-1.19486889,  0.31124418, -1.09138023, ...,  0.44873987,\n",
       "           0.2241919 ,  0.99975959],\n",
       "         [ 0.670171  ,  0.77681352, -1.06566762, ...,  0.07849245,\n",
       "           1.59460183, -0.38711887],\n",
       "         [-0.88402891,  0.64379371, -0.9885298 , ..., -0.5879529 ,\n",
       "          -0.58193159,  0.11369835]]),\n",
       "  'model': LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "   LogisticRegression(C=10000000000.0, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False)],\n",
       "  'test_score': 0.52800000000000002,\n",
       "  'train_score': 0.77666666666666662,\n",
       "  'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  'y_test': 636    -1\n",
       "  1422   -1\n",
       "  15     -1\n",
       "  525    -1\n",
       "  569    -1\n",
       "  1888   -1\n",
       "  528     1\n",
       "  66     -1\n",
       "  20     -1\n",
       "  1671    1\n",
       "  965     1\n",
       "  1444    1\n",
       "  966    -1\n",
       "  1782   -1\n",
       "  669    -1\n",
       "  666     1\n",
       "  869    -1\n",
       "  1049    1\n",
       "  704    -1\n",
       "  1948    1\n",
       "  1083    1\n",
       "  36     -1\n",
       "  959     1\n",
       "  1524   -1\n",
       "  1364    1\n",
       "  740     1\n",
       "  1283    1\n",
       "  1312    1\n",
       "  897     1\n",
       "  1745    1\n",
       "         ..\n",
       "  98     -1\n",
       "  651     1\n",
       "  327     1\n",
       "  1666    1\n",
       "  1905    1\n",
       "  984     1\n",
       "  1704   -1\n",
       "  1220   -1\n",
       "  1693    1\n",
       "  679     1\n",
       "  1305   -1\n",
       "  1085   -1\n",
       "  494    -1\n",
       "  653     1\n",
       "  948     1\n",
       "  919     1\n",
       "  1681    1\n",
       "  1914   -1\n",
       "  1157   -1\n",
       "  1997   -1\n",
       "  848    -1\n",
       "  1404   -1\n",
       "  310    -1\n",
       "  1982   -1\n",
       "  385     1\n",
       "  71      1\n",
       "  931    -1\n",
       "  1872    1\n",
       "  1365    1\n",
       "  738     1\n",
       "  Name: label, dtype: int64,\n",
       "  'y_train': 1229   -1\n",
       "  1994    1\n",
       "  1784   -1\n",
       "  890    -1\n",
       "  874     1\n",
       "  155     1\n",
       "  378     1\n",
       "  888     1\n",
       "  230     1\n",
       "  706     1\n",
       "  789     1\n",
       "  513     1\n",
       "  1223    1\n",
       "  608     1\n",
       "  970    -1\n",
       "  1112   -1\n",
       "  425    -1\n",
       "  321    -1\n",
       "  1246   -1\n",
       "  1377   -1\n",
       "  308     1\n",
       "  782     1\n",
       "  1998    1\n",
       "  742    -1\n",
       "  1019    1\n",
       "  1255   -1\n",
       "  559     1\n",
       "  652     1\n",
       "  1027    1\n",
       "  1399    1\n",
       "         ..\n",
       "  856    -1\n",
       "  1160    1\n",
       "  1332    1\n",
       "  1779    1\n",
       "  1950    1\n",
       "  1383   -1\n",
       "  505    -1\n",
       "  1089    1\n",
       "  1424   -1\n",
       "  932    -1\n",
       "  1861    1\n",
       "  281     1\n",
       "  1523   -1\n",
       "  451     1\n",
       "  939     1\n",
       "  113    -1\n",
       "  1372   -1\n",
       "  1821   -1\n",
       "  1633   -1\n",
       "  298     1\n",
       "  1104   -1\n",
       "  1646    1\n",
       "  1506   -1\n",
       "  1567    1\n",
       "  1373   -1\n",
       "  973     1\n",
       "  411     1\n",
       "  1999    1\n",
       "  450     1\n",
       "  131    -1\n",
       "  Name: label, dtype: int64},\n",
       " {'X_test': array([[-1.35028888, -0.45361972, -0.83425415, ..., -0.66200238,\n",
       "          -1.09247646, -0.65678969],\n",
       "         [-0.10692895,  0.27798923, -0.93710458, ...,  0.52278935,\n",
       "          -1.09247646,  1.61615002],\n",
       "         [ 0.51475101, -0.38710982,  0.63136453, ...,  1.26328418,\n",
       "           0.57351208,  1.88582084],\n",
       "         ..., \n",
       "         [-0.88402891,  1.54167742,  1.71129407, ..., -0.143656  ,\n",
       "           1.70208496,  0.11369835],\n",
       "         [ 0.35933102, -1.5842881 ,  1.4798806 , ..., -0.81010134,\n",
       "           0.41228738, -0.23302127],\n",
       "         [ 0.35933102,  1.17587295,  0.65707713, ...,  0.22659142,\n",
       "          -0.34009454,  1.34647921]]),\n",
       "  'X_train': array([[ 1.13643097,  0.61053876, -0.21715155, ..., -0.36580445,\n",
       "          -1.76424604,  0.57599117],\n",
       "         [ 0.20391103, -0.58663953,  1.96842015, ...,  0.59683883,\n",
       "           1.64834339, -1.46580213],\n",
       "         [ 0.04849104,  0.31124418,  1.09419147, ..., -0.88415083,\n",
       "           1.27215243,  0.57599117],\n",
       "         ..., \n",
       "         [-1.19486889,  0.31124418, -1.09138023, ...,  0.44873987,\n",
       "           0.2241919 ,  0.99975959],\n",
       "         [ 0.670171  ,  0.77681352, -1.06566762, ...,  0.07849245,\n",
       "           1.59460183, -0.38711887],\n",
       "         [-0.88402891,  0.64379371, -0.9885298 , ..., -0.5879529 ,\n",
       "          -0.58193159,  0.11369835]]),\n",
       "  'model': LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "   LogisticRegression(C=10000000000.0, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False)],\n",
       "  'test_score': 0.52800000000000002,\n",
       "  'train_score': 0.77666666666666662,\n",
       "  'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  'y_test': 636    -1\n",
       "  1422   -1\n",
       "  15     -1\n",
       "  525    -1\n",
       "  569    -1\n",
       "  1888   -1\n",
       "  528     1\n",
       "  66     -1\n",
       "  20     -1\n",
       "  1671    1\n",
       "  965     1\n",
       "  1444    1\n",
       "  966    -1\n",
       "  1782   -1\n",
       "  669    -1\n",
       "  666     1\n",
       "  869    -1\n",
       "  1049    1\n",
       "  704    -1\n",
       "  1948    1\n",
       "  1083    1\n",
       "  36     -1\n",
       "  959     1\n",
       "  1524   -1\n",
       "  1364    1\n",
       "  740     1\n",
       "  1283    1\n",
       "  1312    1\n",
       "  897     1\n",
       "  1745    1\n",
       "         ..\n",
       "  98     -1\n",
       "  651     1\n",
       "  327     1\n",
       "  1666    1\n",
       "  1905    1\n",
       "  984     1\n",
       "  1704   -1\n",
       "  1220   -1\n",
       "  1693    1\n",
       "  679     1\n",
       "  1305   -1\n",
       "  1085   -1\n",
       "  494    -1\n",
       "  653     1\n",
       "  948     1\n",
       "  919     1\n",
       "  1681    1\n",
       "  1914   -1\n",
       "  1157   -1\n",
       "  1997   -1\n",
       "  848    -1\n",
       "  1404   -1\n",
       "  310    -1\n",
       "  1982   -1\n",
       "  385     1\n",
       "  71      1\n",
       "  931    -1\n",
       "  1872    1\n",
       "  1365    1\n",
       "  738     1\n",
       "  Name: label, dtype: int64,\n",
       "  'y_train': 1229   -1\n",
       "  1994    1\n",
       "  1784   -1\n",
       "  890    -1\n",
       "  874     1\n",
       "  155     1\n",
       "  378     1\n",
       "  888     1\n",
       "  230     1\n",
       "  706     1\n",
       "  789     1\n",
       "  513     1\n",
       "  1223    1\n",
       "  608     1\n",
       "  970    -1\n",
       "  1112   -1\n",
       "  425    -1\n",
       "  321    -1\n",
       "  1246   -1\n",
       "  1377   -1\n",
       "  308     1\n",
       "  782     1\n",
       "  1998    1\n",
       "  742    -1\n",
       "  1019    1\n",
       "  1255   -1\n",
       "  559     1\n",
       "  652     1\n",
       "  1027    1\n",
       "  1399    1\n",
       "         ..\n",
       "  856    -1\n",
       "  1160    1\n",
       "  1332    1\n",
       "  1779    1\n",
       "  1950    1\n",
       "  1383   -1\n",
       "  505    -1\n",
       "  1089    1\n",
       "  1424   -1\n",
       "  932    -1\n",
       "  1861    1\n",
       "  281     1\n",
       "  1523   -1\n",
       "  451     1\n",
       "  939     1\n",
       "  113    -1\n",
       "  1372   -1\n",
       "  1821   -1\n",
       "  1633   -1\n",
       "  298     1\n",
       "  1104   -1\n",
       "  1646    1\n",
       "  1506   -1\n",
       "  1567    1\n",
       "  1373   -1\n",
       "  973     1\n",
       "  411     1\n",
       "  1999    1\n",
       "  450     1\n",
       "  131    -1\n",
       "  Name: label, dtype: int64},\n",
       " {'X_test': array([[-1.35028888, -0.45361972, -0.83425415, ..., -0.66200238,\n",
       "          -1.09247646, -0.65678969],\n",
       "         [-0.10692895,  0.27798923, -0.93710458, ...,  0.52278935,\n",
       "          -1.09247646,  1.61615002],\n",
       "         [ 0.51475101, -0.38710982,  0.63136453, ...,  1.26328418,\n",
       "           0.57351208,  1.88582084],\n",
       "         ..., \n",
       "         [-0.88402891,  1.54167742,  1.71129407, ..., -0.143656  ,\n",
       "           1.70208496,  0.11369835],\n",
       "         [ 0.35933102, -1.5842881 ,  1.4798806 , ..., -0.81010134,\n",
       "           0.41228738, -0.23302127],\n",
       "         [ 0.35933102,  1.17587295,  0.65707713, ...,  0.22659142,\n",
       "          -0.34009454,  1.34647921]]),\n",
       "  'X_train': array([[ 1.13643097,  0.61053876, -0.21715155, ..., -0.36580445,\n",
       "          -1.76424604,  0.57599117],\n",
       "         [ 0.20391103, -0.58663953,  1.96842015, ...,  0.59683883,\n",
       "           1.64834339, -1.46580213],\n",
       "         [ 0.04849104,  0.31124418,  1.09419147, ..., -0.88415083,\n",
       "           1.27215243,  0.57599117],\n",
       "         ..., \n",
       "         [-1.19486889,  0.31124418, -1.09138023, ...,  0.44873987,\n",
       "           0.2241919 ,  0.99975959],\n",
       "         [ 0.670171  ,  0.77681352, -1.06566762, ...,  0.07849245,\n",
       "           1.59460183, -0.38711887],\n",
       "         [-0.88402891,  0.64379371, -0.9885298 , ..., -0.5879529 ,\n",
       "          -0.58193159,  0.11369835]]),\n",
       "  'model': LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "   LogisticRegression(C=10000000000.0, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False)],\n",
       "  'test_score': 0.52800000000000002,\n",
       "  'train_score': 0.77666666666666662,\n",
       "  'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  'y_test': 636    -1\n",
       "  1422   -1\n",
       "  15     -1\n",
       "  525    -1\n",
       "  569    -1\n",
       "  1888   -1\n",
       "  528     1\n",
       "  66     -1\n",
       "  20     -1\n",
       "  1671    1\n",
       "  965     1\n",
       "  1444    1\n",
       "  966    -1\n",
       "  1782   -1\n",
       "  669    -1\n",
       "  666     1\n",
       "  869    -1\n",
       "  1049    1\n",
       "  704    -1\n",
       "  1948    1\n",
       "  1083    1\n",
       "  36     -1\n",
       "  959     1\n",
       "  1524   -1\n",
       "  1364    1\n",
       "  740     1\n",
       "  1283    1\n",
       "  1312    1\n",
       "  897     1\n",
       "  1745    1\n",
       "         ..\n",
       "  98     -1\n",
       "  651     1\n",
       "  327     1\n",
       "  1666    1\n",
       "  1905    1\n",
       "  984     1\n",
       "  1704   -1\n",
       "  1220   -1\n",
       "  1693    1\n",
       "  679     1\n",
       "  1305   -1\n",
       "  1085   -1\n",
       "  494    -1\n",
       "  653     1\n",
       "  948     1\n",
       "  919     1\n",
       "  1681    1\n",
       "  1914   -1\n",
       "  1157   -1\n",
       "  1997   -1\n",
       "  848    -1\n",
       "  1404   -1\n",
       "  310    -1\n",
       "  1982   -1\n",
       "  385     1\n",
       "  71      1\n",
       "  931    -1\n",
       "  1872    1\n",
       "  1365    1\n",
       "  738     1\n",
       "  Name: label, dtype: int64,\n",
       "  'y_train': 1229   -1\n",
       "  1994    1\n",
       "  1784   -1\n",
       "  890    -1\n",
       "  874     1\n",
       "  155     1\n",
       "  378     1\n",
       "  888     1\n",
       "  230     1\n",
       "  706     1\n",
       "  789     1\n",
       "  513     1\n",
       "  1223    1\n",
       "  608     1\n",
       "  970    -1\n",
       "  1112   -1\n",
       "  425    -1\n",
       "  321    -1\n",
       "  1246   -1\n",
       "  1377   -1\n",
       "  308     1\n",
       "  782     1\n",
       "  1998    1\n",
       "  742    -1\n",
       "  1019    1\n",
       "  1255   -1\n",
       "  559     1\n",
       "  652     1\n",
       "  1027    1\n",
       "  1399    1\n",
       "         ..\n",
       "  856    -1\n",
       "  1160    1\n",
       "  1332    1\n",
       "  1779    1\n",
       "  1950    1\n",
       "  1383   -1\n",
       "  505    -1\n",
       "  1089    1\n",
       "  1424   -1\n",
       "  932    -1\n",
       "  1861    1\n",
       "  281     1\n",
       "  1523   -1\n",
       "  451     1\n",
       "  939     1\n",
       "  113    -1\n",
       "  1372   -1\n",
       "  1821   -1\n",
       "  1633   -1\n",
       "  298     1\n",
       "  1104   -1\n",
       "  1646    1\n",
       "  1506   -1\n",
       "  1567    1\n",
       "  1373   -1\n",
       "  973     1\n",
       "  411     1\n",
       "  1999    1\n",
       "  450     1\n",
       "  131    -1\n",
       "  Name: label, dtype: int64},\n",
       " {'X_test': array([[-1.35028888, -0.45361972, -0.83425415, ..., -0.66200238,\n",
       "          -1.09247646, -0.65678969],\n",
       "         [-0.10692895,  0.27798923, -0.93710458, ...,  0.52278935,\n",
       "          -1.09247646,  1.61615002],\n",
       "         [ 0.51475101, -0.38710982,  0.63136453, ...,  1.26328418,\n",
       "           0.57351208,  1.88582084],\n",
       "         ..., \n",
       "         [-0.88402891,  1.54167742,  1.71129407, ..., -0.143656  ,\n",
       "           1.70208496,  0.11369835],\n",
       "         [ 0.35933102, -1.5842881 ,  1.4798806 , ..., -0.81010134,\n",
       "           0.41228738, -0.23302127],\n",
       "         [ 0.35933102,  1.17587295,  0.65707713, ...,  0.22659142,\n",
       "          -0.34009454,  1.34647921]]),\n",
       "  'X_train': array([[ 1.13643097,  0.61053876, -0.21715155, ..., -0.36580445,\n",
       "          -1.76424604,  0.57599117],\n",
       "         [ 0.20391103, -0.58663953,  1.96842015, ...,  0.59683883,\n",
       "           1.64834339, -1.46580213],\n",
       "         [ 0.04849104,  0.31124418,  1.09419147, ..., -0.88415083,\n",
       "           1.27215243,  0.57599117],\n",
       "         ..., \n",
       "         [-1.19486889,  0.31124418, -1.09138023, ...,  0.44873987,\n",
       "           0.2241919 ,  0.99975959],\n",
       "         [ 0.670171  ,  0.77681352, -1.06566762, ...,  0.07849245,\n",
       "           1.59460183, -0.38711887],\n",
       "         [-0.88402891,  0.64379371, -0.9885298 , ..., -0.5879529 ,\n",
       "          -0.58193159,  0.11369835]]),\n",
       "  'model': LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "   LogisticRegression(C=10000000000.0, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False)],\n",
       "  'test_score': 0.52800000000000002,\n",
       "  'train_score': 0.77666666666666662,\n",
       "  'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  'y_test': 636    -1\n",
       "  1422   -1\n",
       "  15     -1\n",
       "  525    -1\n",
       "  569    -1\n",
       "  1888   -1\n",
       "  528     1\n",
       "  66     -1\n",
       "  20     -1\n",
       "  1671    1\n",
       "  965     1\n",
       "  1444    1\n",
       "  966    -1\n",
       "  1782   -1\n",
       "  669    -1\n",
       "  666     1\n",
       "  869    -1\n",
       "  1049    1\n",
       "  704    -1\n",
       "  1948    1\n",
       "  1083    1\n",
       "  36     -1\n",
       "  959     1\n",
       "  1524   -1\n",
       "  1364    1\n",
       "  740     1\n",
       "  1283    1\n",
       "  1312    1\n",
       "  897     1\n",
       "  1745    1\n",
       "         ..\n",
       "  98     -1\n",
       "  651     1\n",
       "  327     1\n",
       "  1666    1\n",
       "  1905    1\n",
       "  984     1\n",
       "  1704   -1\n",
       "  1220   -1\n",
       "  1693    1\n",
       "  679     1\n",
       "  1305   -1\n",
       "  1085   -1\n",
       "  494    -1\n",
       "  653     1\n",
       "  948     1\n",
       "  919     1\n",
       "  1681    1\n",
       "  1914   -1\n",
       "  1157   -1\n",
       "  1997   -1\n",
       "  848    -1\n",
       "  1404   -1\n",
       "  310    -1\n",
       "  1982   -1\n",
       "  385     1\n",
       "  71      1\n",
       "  931    -1\n",
       "  1872    1\n",
       "  1365    1\n",
       "  738     1\n",
       "  Name: label, dtype: int64,\n",
       "  'y_train': 1229   -1\n",
       "  1994    1\n",
       "  1784   -1\n",
       "  890    -1\n",
       "  874     1\n",
       "  155     1\n",
       "  378     1\n",
       "  888     1\n",
       "  230     1\n",
       "  706     1\n",
       "  789     1\n",
       "  513     1\n",
       "  1223    1\n",
       "  608     1\n",
       "  970    -1\n",
       "  1112   -1\n",
       "  425    -1\n",
       "  321    -1\n",
       "  1246   -1\n",
       "  1377   -1\n",
       "  308     1\n",
       "  782     1\n",
       "  1998    1\n",
       "  742    -1\n",
       "  1019    1\n",
       "  1255   -1\n",
       "  559     1\n",
       "  652     1\n",
       "  1027    1\n",
       "  1399    1\n",
       "         ..\n",
       "  856    -1\n",
       "  1160    1\n",
       "  1332    1\n",
       "  1779    1\n",
       "  1950    1\n",
       "  1383   -1\n",
       "  505    -1\n",
       "  1089    1\n",
       "  1424   -1\n",
       "  932    -1\n",
       "  1861    1\n",
       "  281     1\n",
       "  1523   -1\n",
       "  451     1\n",
       "  939     1\n",
       "  113    -1\n",
       "  1372   -1\n",
       "  1821   -1\n",
       "  1633   -1\n",
       "  298     1\n",
       "  1104   -1\n",
       "  1646    1\n",
       "  1506   -1\n",
       "  1567    1\n",
       "  1373   -1\n",
       "  973     1\n",
       "  411     1\n",
       "  1999    1\n",
       "  450     1\n",
       "  131    -1\n",
       "  Name: label, dtype: int64},\n",
       " {'X_test': array([[-1.35028888, -0.45361972, -0.83425415, ..., -0.66200238,\n",
       "          -1.09247646, -0.65678969],\n",
       "         [-0.10692895,  0.27798923, -0.93710458, ...,  0.52278935,\n",
       "          -1.09247646,  1.61615002],\n",
       "         [ 0.51475101, -0.38710982,  0.63136453, ...,  1.26328418,\n",
       "           0.57351208,  1.88582084],\n",
       "         ..., \n",
       "         [-0.88402891,  1.54167742,  1.71129407, ..., -0.143656  ,\n",
       "           1.70208496,  0.11369835],\n",
       "         [ 0.35933102, -1.5842881 ,  1.4798806 , ..., -0.81010134,\n",
       "           0.41228738, -0.23302127],\n",
       "         [ 0.35933102,  1.17587295,  0.65707713, ...,  0.22659142,\n",
       "          -0.34009454,  1.34647921]]),\n",
       "  'X_train': array([[ 1.13643097,  0.61053876, -0.21715155, ..., -0.36580445,\n",
       "          -1.76424604,  0.57599117],\n",
       "         [ 0.20391103, -0.58663953,  1.96842015, ...,  0.59683883,\n",
       "           1.64834339, -1.46580213],\n",
       "         [ 0.04849104,  0.31124418,  1.09419147, ..., -0.88415083,\n",
       "           1.27215243,  0.57599117],\n",
       "         ..., \n",
       "         [-1.19486889,  0.31124418, -1.09138023, ...,  0.44873987,\n",
       "           0.2241919 ,  0.99975959],\n",
       "         [ 0.670171  ,  0.77681352, -1.06566762, ...,  0.07849245,\n",
       "           1.59460183, -0.38711887],\n",
       "         [-0.88402891,  0.64379371, -0.9885298 , ..., -0.5879529 ,\n",
       "          -0.58193159,  0.11369835]]),\n",
       "  'model': LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "   LogisticRegression(C=10000000000.0, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.055, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False)],\n",
       "  'test_score': 0.52800000000000002,\n",
       "  'train_score': 0.77666666666666662,\n",
       "  'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  'y_test': 636    -1\n",
       "  1422   -1\n",
       "  15     -1\n",
       "  525    -1\n",
       "  569    -1\n",
       "  1888   -1\n",
       "  528     1\n",
       "  66     -1\n",
       "  20     -1\n",
       "  1671    1\n",
       "  965     1\n",
       "  1444    1\n",
       "  966    -1\n",
       "  1782   -1\n",
       "  669    -1\n",
       "  666     1\n",
       "  869    -1\n",
       "  1049    1\n",
       "  704    -1\n",
       "  1948    1\n",
       "  1083    1\n",
       "  36     -1\n",
       "  959     1\n",
       "  1524   -1\n",
       "  1364    1\n",
       "  740     1\n",
       "  1283    1\n",
       "  1312    1\n",
       "  897     1\n",
       "  1745    1\n",
       "         ..\n",
       "  98     -1\n",
       "  651     1\n",
       "  327     1\n",
       "  1666    1\n",
       "  1905    1\n",
       "  984     1\n",
       "  1704   -1\n",
       "  1220   -1\n",
       "  1693    1\n",
       "  679     1\n",
       "  1305   -1\n",
       "  1085   -1\n",
       "  494    -1\n",
       "  653     1\n",
       "  948     1\n",
       "  919     1\n",
       "  1681    1\n",
       "  1914   -1\n",
       "  1157   -1\n",
       "  1997   -1\n",
       "  848    -1\n",
       "  1404   -1\n",
       "  310    -1\n",
       "  1982   -1\n",
       "  385     1\n",
       "  71      1\n",
       "  931    -1\n",
       "  1872    1\n",
       "  1365    1\n",
       "  738     1\n",
       "  Name: label, dtype: int64,\n",
       "  'y_train': 1229   -1\n",
       "  1994    1\n",
       "  1784   -1\n",
       "  890    -1\n",
       "  874     1\n",
       "  155     1\n",
       "  378     1\n",
       "  888     1\n",
       "  230     1\n",
       "  706     1\n",
       "  789     1\n",
       "  513     1\n",
       "  1223    1\n",
       "  608     1\n",
       "  970    -1\n",
       "  1112   -1\n",
       "  425    -1\n",
       "  321    -1\n",
       "  1246   -1\n",
       "  1377   -1\n",
       "  308     1\n",
       "  782     1\n",
       "  1998    1\n",
       "  742    -1\n",
       "  1019    1\n",
       "  1255   -1\n",
       "  559     1\n",
       "  652     1\n",
       "  1027    1\n",
       "  1399    1\n",
       "         ..\n",
       "  856    -1\n",
       "  1160    1\n",
       "  1332    1\n",
       "  1779    1\n",
       "  1950    1\n",
       "  1383   -1\n",
       "  505    -1\n",
       "  1089    1\n",
       "  1424   -1\n",
       "  932    -1\n",
       "  1861    1\n",
       "  281     1\n",
       "  1523   -1\n",
       "  451     1\n",
       "  939     1\n",
       "  113    -1\n",
       "  1372   -1\n",
       "  1821   -1\n",
       "  1633   -1\n",
       "  298     1\n",
       "  1104   -1\n",
       "  1646    1\n",
       "  1506   -1\n",
       "  1567    1\n",
       "  1373   -1\n",
       "  973     1\n",
       "  411     1\n",
       "  1999    1\n",
       "  450     1\n",
       "  131    -1\n",
       "  Name: label, dtype: int64}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
